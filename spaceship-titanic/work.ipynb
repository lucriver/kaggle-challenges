{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Space-ship Titanic Kaggle Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This work is my own. I didn't reference any existing guides for this kaggle competition to get my score.\n",
    "- Kaggle score for this work: 0.80336 (position 550/2543)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HSA_OVERRIDE_GFX_VERSION'] = '10.3.0'\n",
    "\n",
    "# basic modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Read Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "data_path = './data/'\n",
    "train = 'train.csv'\n",
    "\n",
    "train_file = os.path.join(data_path,train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Column Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PassengerId Column\n",
    "**Engineer 'PassengerId' column**:\n",
    "- The 'PassengerId' column is of the form: gggg_pp where gggg indicates the group they are traveling with and pp is their number within the group. By extracting these values we can create meaning from this column and hopefully use it to impute missing values later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GroupId'] = df['PassengerId'].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "df['PersonId'] = df['PassengerId'].apply(lambda x: x.split(\"_\")[1]).astype(int)\n",
    "df.drop(columns=['PassengerId'], inplace=True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Cabin' Column\n",
    "**Engineer the 'Cabin' column:**\n",
    "- The 'Cabin' column is of the form deck/num/side. We can engineer this feature to extract meaningful info for the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Deck'] = df[df['Cabin'].notna()]['Cabin'].str.split('/').apply(lambda x: x[0])\n",
    "df['Num'] = df[df['Cabin'].notna()]['Cabin'].str.split('/').apply(lambda x: x[1])\n",
    "df['Side'] = df[df['Cabin'].notna()]['Cabin'].str.split('/').apply(lambda x: x[2])\n",
    "df.drop(columns=['Cabin'],inplace=True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Column\n",
    "**First Name and Last Name extraction**\n",
    "- By extracting the first and last names, we can hopefully use this data to impute missing features. Namely, we can hopefully use last names to impute missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FirstName'] = df[df['Name'].notna()]['Name'].str.split(' ').apply(lambda x: x[0].strip())\n",
    "df['LastName'] = df[df['Name'].notna()]['Name'].str.split(' ').apply(lambda x: x[1].strip())\n",
    "df.drop(columns=['Name'], inplace = True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation: Missing Home Planets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Missing home planet imputation**\n",
    "- Imputation of missing home planets might be relatively simple to do. We might only have to consider shared attribtues among groups we are confident about like shared last names, group ids and home planets.\n",
    "- Before we attempt to impute missing home-planets, we should have some confidence that these missing home planets are in-fact MCAR, so we will run a chi-2 test on the home planets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency = pd.crosstab(df['HomePlanet'], df['Transported'], dropna=False)\n",
    "\n",
    "print(contingency)\n",
    "\n",
    "c, p, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "print(c,p,dof)\n",
    "print(expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation\n",
    "- We can see that the frequency table for the missing home planet and the chi-2 expected frequency values are nearly the same, which indicates that there isn't an apparent association between a home planet being missing and being successfully transported or not. This hints at homeplanet being missing being an MCAR value, which means we will proceed with imputation of this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing home planets (High-Confidence) (Family members)\n",
    "Missing home planet will be imputed for groups of people such that:\n",
    "- They all have the same GroupID\n",
    "- All have the same LastName\n",
    "- All come from the same planet\n",
    "- All going to the same destination\n",
    "\n",
    "The home planet imputed will be the groups home planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groups_0(df) -> list:\n",
    "  groups = []\n",
    "  group_ids = df['GroupId'].unique().tolist()\n",
    "  for group_id in group_ids:\n",
    "\n",
    "    # get sub-dataframe based off of group id\n",
    "    group_df = df[df['GroupId'] == group_id]\n",
    "\n",
    "    has_missing_planet = group_df['HomePlanet'].isna().any()\n",
    "    has_one_distinct_home = group_df['HomePlanet'].dropna().nunique() == 1\n",
    "    has_one_distinct_destination = group_df['Destination'].dropna().nunique() == 1\n",
    "    has_one_distinct_last_name = group_df['LastName'].dropna().nunique() == 1\n",
    "    \n",
    "    if (\n",
    "        has_missing_planet and\n",
    "        has_one_distinct_last_name and\n",
    "        has_one_distinct_home and\n",
    "        has_one_distinct_destination        \n",
    "    ):\n",
    "        groups.append(group_df)\n",
    "      \n",
    "  return groups\n",
    "\n",
    "group_dfs = get_groups_0(df)\n",
    "\n",
    "print(f\"number of samples where home planet is missing:{df['HomePlanet'].isna().sum()}\")\n",
    "print(f\"Number of groups: {len(group_dfs)}\")\n",
    "\n",
    "while group_dfs:\n",
    "  group_df = group_dfs.pop()\n",
    "  home_planets = group_df['HomePlanet'].dropna().unique()\n",
    "  if len(home_planets) > 1:\n",
    "    raise ValueError(home_planets)\n",
    "  home_planet = home_planets[0]\n",
    "  df.loc[group_df['HomePlanet'].isna().index, 'HomePlanet'] = home_planet\n",
    "  \n",
    "print(f\"number of samples where home planet is missing:{df['HomePlanet'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing home planets (Medium Confidence) (Family members from same planet)\n",
    "**Imput Home Planets by:**\n",
    "  - Groups where GroupID are all the same\n",
    "  - Groups where LastName are all the same\n",
    "  - There is only one unique type of home planet in the group of people\n",
    "  \n",
    "Update missing home planets with the groups single distinct non-na home planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groups_1(df):\n",
    "    groups =[]\n",
    "    group_ids = df['GroupId'].unique().tolist()\n",
    "    for group_id in group_ids:\n",
    "        group_df = df[df['GroupId'] == group_id]\n",
    "        \n",
    "        missing_home_planet = group_df['HomePlanet'].isna().any()\n",
    "        one_distinct_last_name = group_df['LastName'].dropna().nunique() == 1\n",
    "        one_distinct_home_planet = group_df['HomePlanet'].dropna().nunique() == 1\n",
    "        \n",
    "        if (\n",
    "            missing_home_planet and\n",
    "            one_distinct_last_name and\n",
    "            one_distinct_home_planet\n",
    "        ):\n",
    "            groups.append(group_df)\n",
    "            \n",
    "    return groups\n",
    "\n",
    "group_dfs = get_groups_1(df)\n",
    "\n",
    "print(f\"number of samples with missing home planet: {df['HomePlanet'].isna().sum()}\")\n",
    "print(\"groups:\",len(group_dfs))\n",
    "\n",
    "while group_dfs:\n",
    "    group_df = group_dfs.pop()\n",
    "    \n",
    "    planets = group_df['HomePlanet'].dropna().unique().tolist()\n",
    "    \n",
    "    if len(planets) != 1:\n",
    "        raise ValueError(\"HUH\")\n",
    "\n",
    "    df.loc[group_df['HomePlanet'].isna().index, 'HomePlanet'] = planets[0]    \n",
    "    \n",
    "print(f'samples remaining with missing home planets: {df[\"HomePlanet\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation: Missing destination planets\n",
    "- Same process as with missing home planets essentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency = pd.crosstab(df['Destination'], df['Transported'], dropna=False)\n",
    "\n",
    "print(contingency)\n",
    "\n",
    "c, p, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "print(c,p,dof)\n",
    "print(expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obvservation:\n",
    "- Similar case as the home planetplanet feature. These missing destination planets appear to be MCAR due to the low assocation. We will impute the missing values.\n",
    "\n",
    "Action\n",
    "- For Groups of GroupId such that:\n",
    "    - There is at least one person in the group with a missing destination planet\n",
    "    - All people in the group have the same last name (family)\n",
    "    - ALl people in the group have the same home planet\n",
    "    - ALl people in the group have the same destination (excluding the missing destinations)\n",
    "Fill the missing destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_2(df):\n",
    "  groups = []\n",
    "  group_ids = df['GroupId'].unique().tolist()\n",
    "    \n",
    "  for group_id in group_ids:\n",
    "    \n",
    "    group_df = df[df['GroupId'] == group_id]\n",
    "    \n",
    "    at_least_one_missing_destination_planet = group_df['Destination'].isna().any()\n",
    "    only_one_distinct_destination_planet = group_df['Destination'].dropna().nunique() == 1    \n",
    "    only_one_distinct_home_planet = group_df['HomePlanet'].dropna().nunique() == 1\n",
    "    only_one_distinct_last_name = group_df['LastName'].dropna().nunique() == 1\n",
    "    \n",
    "    if (\n",
    "        at_least_one_missing_destination_planet and\n",
    "        only_one_distinct_home_planet and\n",
    "        only_one_distinct_destination_planet and\n",
    "        only_one_distinct_last_name\n",
    "    ):\n",
    "        groups.append(group_df)\n",
    "    \n",
    "  return groups\n",
    "\n",
    "print('number of remaining samples where destination is missing:',df['Destination'].isna().sum())\n",
    "\n",
    "groups = get_group_2(df)\n",
    "print(f'number of groups found: {len(groups)}')\n",
    "\n",
    "while groups:\n",
    "    group_df = groups.pop()\n",
    "    destination_planets = group_df['Destination'].dropna().unique().tolist()\n",
    "    if len(destination_planets) != 1:\n",
    "        raise ValueError(len(destination_planets))\n",
    "    df.loc[group_df['Destination'].isna().index, 'Destination'] = destination_planets[0]\n",
    "    \n",
    "print('number of remaining samples where destination is missing after imputation:',df['Destination'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "To determine which categorical features to impute next, lets attempt to rank our categorical features importance with respect to the target using regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(df.info())\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your data is in a DataFrame called 'df'\n",
    "categorical_features = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Side']  # Replace with your categorical feature names\n",
    "continuous_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'GroupId', 'Num', 'PersonId']\n",
    "target_feature = 'Transported'  # Replace with the name of your target variable\n",
    "\n",
    "temp_df = df[categorical_features + continuous_features + [target_feature]]\n",
    "\n",
    "temp_df = pd.get_dummies(temp_df, columns=categorical_features)\n",
    "\n",
    "X = temp_df.drop(columns=[target_feature])\n",
    "y = temp_df[target_feature]\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X, y)\n",
    "\n",
    "importance_df = pd.DataFrame({'Feature': rf_classifier.feature_names_in_, 'Importance': rf_classifier.feature_importances_})\n",
    "\n",
    "# Sort the features by importance in descending order\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance (Random Forest) for Transported')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- Continuous features have the largest impact on the target.\n",
    "- Unexpectedly, 'Num', and 'GroupId' have a large impact on the target.\n",
    "- The categorical feature that has the largest impact on the target is CryoSleep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cryosleep Analysis\n",
    "- Let's try to glean information about cryosleep for imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "did_cryo = df[df['CryoSleep'] == True]\n",
    "no_cryo = df[df['CryoSleep'] == False]\n",
    "\n",
    "spending_money_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "display(\"DID CRYO\")\n",
    "display(did_cryo.head())\n",
    "display(did_cryo[spending_money_cols].describe())\n",
    "display('-----------')\n",
    "display('NO CRYO')\n",
    "display(no_cryo.head())\n",
    "display(no_cryo[spending_money_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- We can see that people who do cryo never spend any money. This can help us impute missing cryo data and missing money data. \n",
    "- We can see that people who did CRYO and were VIP were all from Europa Home planet. We can quickly impute this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation: CryoSleep and Shopping Data\n",
    "- We discussed earlier that shopping data is a good proxy for determining if someone did cryosleep or not. Lets begin by imputing shopping data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Imputation:***\n",
    "- If someone did cryosleep, we know they didn't spend any money. So, for any sample where shopping data is missing, if the person did cryosleep, we will impute a 0.0.\n",
    "- If cryosleep data is missing, and we see the person spent money, we know they didn't do cryosleep. That is, if a person spent money and cryosleep is missing, we will fill a False.df[(df['CryoSleep'] == True) & (df[spending_money_cols].sum(axis=1))]spending_money_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spending_money_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If cryosleep is missing, and money was spent, that person did not do cryosleep, so impute cryosleep with false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# impute missing cryosleep columns with false (pretty safe)\n",
    "mask = df[(df['CryoSleep'].isna()) & (df[spending_money_cols].sum(axis=1) > 0.0)].index\n",
    "df.loc[mask, 'CryoSleep'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If someone did cryosleep, and any spending columns are missing, impute those with zero because they did cryosleep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# impute spending columns (somewhat risky)\n",
    "# cond_1 = (df['CryoSleep'] == True)\n",
    "# cond_2 = (df[spending_money_cols].sum(axis=1) == 0.0)\n",
    "# cond_3 = (df[spending_money_cols].isna().any(axis=1))\n",
    "# mask = df[cond_1 & cond_2 & cond_3].index\n",
    "# df.loc[mask, spending_money_cols] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sanity check, this should have nothing\n",
    "df[(df['CryoSleep'] == True) & (df[spending_money_cols].sum(axis=1) > 0.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering: TotalSpent and SpentMoney column\n",
    "- Spending money appears to be a good proxy for certain information. We can create a simple continuous feature that simply tells how much money, if any, was spent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Total spent: strict version. THis will only compute the total spent when we have a value for every shopping column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spending_money_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "\n",
    "# total spent - less strict version\n",
    "df['TotalSpent'] = df[spending_money_cols].sum(axis=1)\n",
    "\n",
    "# total spent strict version\n",
    "# cond_1 = (df[spending_money_cols].notna().all(axis=1))\n",
    "# mask = df.loc[cond_1].index\n",
    "# df.loc[mask, 'TotalSpent'] = df.loc[mask][spending_money_cols].sum(axis=1)\n",
    "\n",
    "# display(df.loc[mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Round II: Gradient Boosting\n",
    "- We've imputed some data and engineered some new features. Lets check out the new feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming your data is in a DataFrame called 'df'\n",
    "categorical_features = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Side']  # Replace with your categorical feature names\n",
    "continuous_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'GroupId', 'Num', 'PersonId', 'TotalSpent']\n",
    "target_feature = 'Transported'  # Replace with the name of your target variable\n",
    "\n",
    "temp_df = df[categorical_features + continuous_features + [target_feature]].dropna()\n",
    "\n",
    "temp_df = pd.get_dummies(temp_df, columns=categorical_features)\n",
    "\n",
    "X = temp_df.drop(columns=[target_feature])\n",
    "y = temp_df[target_feature]\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rf_classifier = GradientBoostingClassifier(n_estimators=100, random_state=1337)\n",
    "rf_classifier.fit(X, y)\n",
    "\n",
    "importance_df = pd.DataFrame({'Feature': rf_classifier.feature_names_in_, 'Importance': rf_classifier.feature_importances_})\n",
    "\n",
    "# Sort the features by importance in descending order\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance (Gradient Boosting) for Transported')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Observations:***\n",
    "- 'TotalSpent' has a significant impact on performance.\n",
    "- The continuous spending columns have the second largest impact on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaining Missing Data:\n",
    "- Lets check out what else we can try to impute and analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"number of samples without missing data:\")\n",
    "print(len(df.dropna()))\n",
    "\n",
    "print(\"Missing features total:\")\n",
    "display(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further HomePlanet-based EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "planets = df['HomePlanet'].dropna().unique().tolist()\n",
    "for planet in planets:\n",
    "    print('planet:',planet)\n",
    "    df_planet = df[df['HomePlanet'] == planet]\n",
    "    print(df_planet['VIP'].value_counts(normalize=True))\n",
    "    print(df_planet['Destination'].value_counts(normalize=True))\n",
    "    print(df_planet['CryoSleep'].value_counts(normalize=True))\n",
    "    print(df_planet['Deck'].value_counts(normalize=True))\n",
    "    print(df_planet['Side'].value_counts(normalize=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIP based EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_isvip = df[df['VIP'] == True]\n",
    "df_novip = df[df['VIP'] == False]\n",
    "df_nanvip = df[df['VIP'].isna()]\n",
    "\n",
    "df_vips = [df_isvip, df_novip, df_nanvip]\n",
    "for df_vip in df_vips:\n",
    "    display(df_vip['VIP'].value_counts())\n",
    "    display(df_vip['Deck'].value_counts(normalize=True))\n",
    "    display(df_vip[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].max())\n",
    "    display(df_vip['CryoSleep'].value_counts(normalize=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cryo-sleep based EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cryosleeps = df['CryoSleep'].dropna().unique().tolist()\n",
    "for cryosleep in cryosleeps:\n",
    "    print('cryosleep:',cryosleep)\n",
    "    df_cryo = df[df['CryoSleep'] == cryosleep]\n",
    "    print(df_cryo['VIP'].value_counts(normalize=True))\n",
    "    print(df_cryo['Destination'].value_counts(normalize=True))\n",
    "    print(df_cryo['Deck'].value_counts(normalize=True))\n",
    "    print(df_cryo['Side'].value_counts(normalize=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[(df['CryoSleep'] == True) & (df['VIP'] == True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving on to training\n",
    "- Its not immediately obvious any more correlations between features. Further analysis will be very granular on features that have low feature importance. At this point, we will move on to training the model and hyperparamter tuning. We will revisit if performance is unacceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle score\n",
    "- .80336 (top 550 people!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "Imputations I missed:\n",
    "- The most obvious imputations I missed are related to correlation between age & spending, age & cryosleep. There are some correlations with decks, but i do not feel i could have made any assertions/assumptions with confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
