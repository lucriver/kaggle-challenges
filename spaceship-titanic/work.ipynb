{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Space-ship Titanic Kaggle Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HSA_OVERRIDE_GFX_VERSION'] = '10.3.0'\n",
    "\n",
    "# basic modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Read Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "data_path = './data/'\n",
    "train = 'train.csv'\n",
    "\n",
    "train_file = os.path.join(data_path,train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8693 entries, 0 to 8692\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   PassengerId   8693 non-null   object \n",
      " 1   HomePlanet    8492 non-null   object \n",
      " 2   CryoSleep     8476 non-null   object \n",
      " 3   Cabin         8494 non-null   object \n",
      " 4   Destination   8511 non-null   object \n",
      " 5   Age           8514 non-null   float64\n",
      " 6   VIP           8490 non-null   object \n",
      " 7   RoomService   8512 non-null   float64\n",
      " 8   FoodCourt     8510 non-null   float64\n",
      " 9   ShoppingMall  8485 non-null   float64\n",
      " 10  Spa           8510 non-null   float64\n",
      " 11  VRDeck        8505 non-null   float64\n",
      " 12  Name          8493 non-null   object \n",
      " 13  Transported   8693 non-null   bool   \n",
      "dtypes: bool(1), float64(6), object(7)\n",
      "memory usage: 891.5+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Name</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>B/0/P</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Maham Ofracculy</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Juanna Vines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>58.0</td>\n",
       "      <td>True</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Altark Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003_02</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>33.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>Solam Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/1/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Willy Santantines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  Transported\n",
       "0     0001_01     Europa     False  B/0/P  TRAPPIST-1e  39.0  False          0.0        0.0           0.0     0.0     0.0    Maham Ofracculy        False\n",
       "1     0002_01      Earth     False  F/0/S  TRAPPIST-1e  24.0  False        109.0        9.0          25.0   549.0    44.0       Juanna Vines         True\n",
       "2     0003_01     Europa     False  A/0/S  TRAPPIST-1e  58.0   True         43.0     3576.0           0.0  6715.0    49.0      Altark Susent        False\n",
       "3     0003_02     Europa     False  A/0/S  TRAPPIST-1e  33.0  False          0.0     1283.0         371.0  3329.0   193.0       Solam Susent        False\n",
       "4     0004_01      Earth     False  F/1/S  TRAPPIST-1e  16.0  False        303.0       70.0         151.0   565.0     2.0  Willy Santantines         True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.info())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Column Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PassengerId Column\n",
    "**Engineer 'PassengerId' column**:\n",
    "- The 'PassengerId' column is of the form: gggg_pp where gggg indicates the group they are traveling with and pp is their number within the group. By extracting these values we can create meaning from this column and hopefully use it to impute missing values later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Name</th>\n",
       "      <th>Transported</th>\n",
       "      <th>GroupId</th>\n",
       "      <th>PersonId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>B/0/P</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Maham Ofracculy</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Juanna Vines</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  HomePlanet CryoSleep  Cabin  Destination   Age    VIP  RoomService  FoodCourt  ShoppingMall    Spa  VRDeck             Name  Transported  GroupId  PersonId\n",
       "0     Europa     False  B/0/P  TRAPPIST-1e  39.0  False          0.0        0.0           0.0    0.0     0.0  Maham Ofracculy        False        1         1\n",
       "1      Earth     False  F/0/S  TRAPPIST-1e  24.0  False        109.0        9.0          25.0  549.0    44.0     Juanna Vines         True        2         1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['GroupId'] = df['PassengerId'].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "df['PersonId'] = df['PassengerId'].apply(lambda x: x.split(\"_\")[1]).astype(int)\n",
    "df.drop(columns=['PassengerId'], inplace=True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Cabin' Column\n",
    "**Engineer the 'Cabin' column:**\n",
    "- The 'Cabin' column is of the form deck/num/side. We can engineer this feature to extract meaningful info for the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Name</th>\n",
       "      <th>Transported</th>\n",
       "      <th>GroupId</th>\n",
       "      <th>PersonId</th>\n",
       "      <th>Deck</th>\n",
       "      <th>Num</th>\n",
       "      <th>Side</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Maham Ofracculy</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Juanna Vines</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  HomePlanet CryoSleep  Destination   Age    VIP  RoomService  FoodCourt  ShoppingMall    Spa  VRDeck             Name  Transported  GroupId  PersonId Deck Num Side\n",
       "0     Europa     False  TRAPPIST-1e  39.0  False          0.0        0.0           0.0    0.0     0.0  Maham Ofracculy        False        1         1    B   0    P\n",
       "1      Earth     False  TRAPPIST-1e  24.0  False        109.0        9.0          25.0  549.0    44.0     Juanna Vines         True        2         1    F   0    S"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Deck'] = df[df['Cabin'].notna()]['Cabin'].str.split('/').apply(lambda x: x[0])\n",
    "df['Num'] = df[df['Cabin'].notna()]['Cabin'].str.split('/').apply(lambda x: x[1])\n",
    "df['Side'] = df[df['Cabin'].notna()]['Cabin'].str.split('/').apply(lambda x: x[2])\n",
    "df.drop(columns=['Cabin'],inplace=True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Column\n",
    "**First Name and Last Name extraction**\n",
    "- By extracting the first and last names, we can hopefully use this data to impute missing features. Namely, we can hopefully use last names to impute missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Transported</th>\n",
       "      <th>GroupId</th>\n",
       "      <th>PersonId</th>\n",
       "      <th>Deck</th>\n",
       "      <th>Num</th>\n",
       "      <th>Side</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>LastName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>P</td>\n",
       "      <td>Maham</td>\n",
       "      <td>Ofracculy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>Juanna</td>\n",
       "      <td>Vines</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  HomePlanet CryoSleep  Destination   Age    VIP  RoomService  FoodCourt  ShoppingMall    Spa  VRDeck  Transported  GroupId  PersonId Deck Num Side FirstName   LastName\n",
       "0     Europa     False  TRAPPIST-1e  39.0  False          0.0        0.0           0.0    0.0     0.0        False        1         1    B   0    P     Maham  Ofracculy\n",
       "1      Earth     False  TRAPPIST-1e  24.0  False        109.0        9.0          25.0  549.0    44.0         True        2         1    F   0    S    Juanna      Vines"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['FirstName'] = df[df['Name'].notna()]['Name'].str.split(' ').apply(lambda x: x[0].strip())\n",
    "df['LastName'] = df[df['Name'].notna()]['Name'].str.split(' ').apply(lambda x: x[1].strip())\n",
    "df.drop(columns=['Name'], inplace = True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation: Missing Home Planets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Missing home planet imputation**\n",
    "- Imputation of missing home planets might be relatively simple to do. We might only have to consider shared attribtues among groups we are confident about like shared last names, group ids and home planets.\n",
    "- Before we attempt to impute missing home-planets, we should have some confidence that these missing home planets are in-fact MCAR, so we will run a chi-2 test on the home planets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transported  False  True \n",
      "HomePlanet               \n",
      "Earth         2651   1951\n",
      "Europa         727   1404\n",
      "Mars           839    920\n",
      "NaN             98    103\n",
      "324.96723663979583 3.9214919240932375e-70 3\n",
      "[[2284.32416887 2317.67583113]\n",
      " [1057.77809732 1073.22190268]\n",
      " [ 873.12607845  885.87392155]\n",
      " [  99.77165535  101.22834465]]\n"
     ]
    }
   ],
   "source": [
    "contingency = pd.crosstab(df['HomePlanet'], df['Transported'], dropna=False)\n",
    "\n",
    "print(contingency)\n",
    "\n",
    "c, p, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "print(c,p,dof)\n",
    "print(expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation\n",
    "- We can see that the frequency table for the missing home planet and the chi-2 expected frequency values are nearly the same, which indicates that there isn't an apparent association between a home planet being missing and being successfully transported or not. This hints at homeplanet being missing being an MCAR value, which means we will proceed with imputation of this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing home planets (High-Confidence) (Family members)\n",
    "Missing home planet will be imputed for groups of people such that:\n",
    "- They all have the same GroupID\n",
    "- All have the same LastName\n",
    "- All come from the same planet\n",
    "- All going to the same destination\n",
    "\n",
    "The home planet imputed will be the groups home planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples where home planet is missing:201\n",
      "Number of groups: 36\n",
      "number of samples where home planet is missing:165\n"
     ]
    }
   ],
   "source": [
    "def get_groups_0(df) -> list:\n",
    "  groups = []\n",
    "  group_ids = df['GroupId'].unique().tolist()\n",
    "  for group_id in group_ids:\n",
    "\n",
    "    # get sub-dataframe based off of group id\n",
    "    group_df = df[df['GroupId'] == group_id]\n",
    "\n",
    "    has_missing_planet = group_df['HomePlanet'].isna().any()\n",
    "    has_one_distinct_home = group_df['HomePlanet'].dropna().nunique() == 1\n",
    "    has_one_distinct_destination = group_df['Destination'].dropna().nunique() == 1\n",
    "    has_one_distinct_last_name = group_df['LastName'].dropna().nunique() == 1\n",
    "    \n",
    "    if (\n",
    "        has_missing_planet and\n",
    "        has_one_distinct_last_name and\n",
    "        has_one_distinct_home and\n",
    "        has_one_distinct_destination        \n",
    "    ):\n",
    "        groups.append(group_df)\n",
    "      \n",
    "  return groups\n",
    "\n",
    "group_dfs = get_groups_0(df)\n",
    "print(f\"number of samples where home planet is missing:{df['HomePlanet'].isna().sum()}\")\n",
    "print(f\"Number of groups: {len(group_dfs)}\")\n",
    "\n",
    "while group_dfs:\n",
    "  group_df = group_dfs.pop()\n",
    "  home_planets = group_df['HomePlanet'].dropna().unique()\n",
    "  if len(home_planets) > 1:\n",
    "    raise ValueError(home_planets)\n",
    "  home_planet = home_planets[0]\n",
    "  df.loc[group_df['HomePlanet'].isna().index, 'HomePlanet'] = home_planet\n",
    "  \n",
    "print(f\"number of samples where home planet is missing:{df['HomePlanet'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing home planets (Medium Confidence) (Family members from same planet)\n",
    "**Imput Home Planets by:**\n",
    "  - Groups where GroupID are all the same\n",
    "  - Groups where LastName are all the same\n",
    "  - There is only one unique type of home planet in the group of people\n",
    "  \n",
    "Update missing home planets with the groups single distinct non-na home planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples with missing home planet: 165\n",
      "groups: 31\n",
      "samples remaining with missing home planets: 133\n"
     ]
    }
   ],
   "source": [
    "def get_groups_1(df):\n",
    "    groups =[]\n",
    "    group_ids = df['GroupId'].unique().tolist()\n",
    "    for group_id in group_ids:\n",
    "        group_df = df[df['GroupId'] == group_id]\n",
    "        \n",
    "        missing_home_planet = group_df['HomePlanet'].isna().any()\n",
    "        one_distinct_last_name = group_df['LastName'].dropna().nunique() == 1\n",
    "        one_distinct_home_planet = group_df['HomePlanet'].dropna().nunique() == 1\n",
    "        \n",
    "        if (\n",
    "            missing_home_planet and\n",
    "            one_distinct_last_name and\n",
    "            one_distinct_home_planet\n",
    "        ):\n",
    "            groups.append(group_df)\n",
    "            \n",
    "    return groups\n",
    "\n",
    "group_dfs = get_groups_1(df)\n",
    "\n",
    "print(f\"number of samples with missing home planet: {df['HomePlanet'].isna().sum()}\")\n",
    "print(\"groups:\",len(group_dfs))\n",
    "\n",
    "while group_dfs:\n",
    "    group_df = group_dfs.pop()\n",
    "    \n",
    "    planets = group_df['HomePlanet'].dropna().unique().tolist()\n",
    "    \n",
    "    if len(planets) != 1:\n",
    "        raise ValueError(\"HUH\")\n",
    "\n",
    "    df.loc[group_df['HomePlanet'].isna().index, 'HomePlanet'] = planets[0]    \n",
    "    \n",
    "print(f'samples remaining with missing home planets: {df[\"HomePlanet\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation: Missing destination planets\n",
    "- Same process as with missing home planets essentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transported    False  True \n",
      "Destination                \n",
      "55 Cancri e      702   1098\n",
      "PSO J318.5-22    395    401\n",
      "TRAPPIST-1e     3128   2787\n",
      "NaN               90     92\n",
      "106.39488238823832 6.547434028958798e-23 3\n",
      "[[ 893.47751064  906.52248936]\n",
      " [ 395.11561026  400.88438974]\n",
      " [2936.06637524 2978.93362476]\n",
      " [  90.34050385   91.65949615]]\n"
     ]
    }
   ],
   "source": [
    "contingency = pd.crosstab(df['Destination'], df['Transported'], dropna=False)\n",
    "\n",
    "print(contingency)\n",
    "\n",
    "c, p, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "print(c,p,dof)\n",
    "print(expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obvservation:\n",
    "- Similar case as the home planetplanet feature. These missing destination planets appear to be MCAR due to the low assocation. We will impute the missing values.\n",
    "\n",
    "Action\n",
    "- For Groups of GroupId such that:\n",
    "    - There is at least one person in the group with a missing destination planet\n",
    "    - All people in the group have the same last name (family)\n",
    "    - ALl people in the group have the same home planet\n",
    "    - ALl people in the group have the same destination (excluding the missing destinations)\n",
    "Fill the missing destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of remaining samples where destination is missing: 182\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m groups\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber of remaining samples where destination is missing:\u001b[39m\u001b[38;5;124m'\u001b[39m,df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDestination\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum())\n\u001b[0;32m---> 26\u001b[0m groups \u001b[38;5;241m=\u001b[39m \u001b[43mget_group_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber of groups found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(groups)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m groups:\n",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m, in \u001b[0;36mget_group_2\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      7\u001b[0m group_df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGroupId\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m group_id]\n\u001b[1;32m      9\u001b[0m at_least_one_missing_destination_planet \u001b[38;5;241m=\u001b[39m group_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDestination\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39many()\n\u001b[0;32m---> 10\u001b[0m only_one_distinct_destination_planet \u001b[38;5;241m=\u001b[39m \u001b[43mgroup_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDestination\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnunique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m    \n\u001b[1;32m     11\u001b[0m only_one_distinct_home_planet \u001b[38;5;241m=\u001b[39m group_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHomePlanet\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     12\u001b[0m only_one_distinct_last_name \u001b[38;5;241m=\u001b[39m group_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLastName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Code/kaggle-challenges/venv/lib/python3.10/site-packages/pandas/core/base.py:1063\u001b[0m, in \u001b[0;36mIndexOpsMixin.nunique\u001b[0;34m(self, dropna)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnunique\u001b[39m(\u001b[38;5;28mself\u001b[39m, dropna: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;124;03m    Return number of unique elements in the object.\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;124;03m    4\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1063\u001b[0m     uniqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dropna:\n\u001b[1;32m   1065\u001b[0m         uniqs \u001b[38;5;241m=\u001b[39m remove_na_arraylike(uniqs)\n",
      "File \u001b[0;32m~/Code/kaggle-challenges/venv/lib/python3.10/site-packages/pandas/core/series.py:2407\u001b[0m, in \u001b[0;36mSeries.unique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:  \u001b[38;5;66;03m# pylint: disable=useless-parent-delegation\u001b[39;00m\n\u001b[1;32m   2345\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;124;03m    Return unique values of Series object.\u001b[39;00m\n\u001b[1;32m   2347\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2405\u001b[0m \u001b[38;5;124;03m    Categories (3, object): ['a' < 'b' < 'c']\u001b[39;00m\n\u001b[1;32m   2406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/kaggle-challenges/venv/lib/python3.10/site-packages/pandas/core/base.py:1020\u001b[0m, in \u001b[0;36mIndexOpsMixin.unique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1020\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m   1022\u001b[0m         \u001b[38;5;66;03m# i.e. ExtensionArray\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m         result \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39munique()\n",
      "File \u001b[0;32m~/Code/kaggle-challenges/venv/lib/python3.10/site-packages/pandas/core/series.py:831\u001b[0m, in \u001b[0;36mSeries._values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;124;03m    Return Series as ndarray or ndarray-like depending on the dtype.\u001b[39;00m\n\u001b[1;32m    793\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;124;03m           '2013-01-03T05:00:00.000000000'], dtype='datetime64[ns]')\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mexternal_values()\n\u001b[0;32m--> 831\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_values\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    833\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;124;03m    Return the internal repr of this data (defined by Block.interval_values).\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;124;03m    This are the values as stored in the Block (ndarray or ExtensionArray\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    861\u001b[0m \n\u001b[1;32m    862\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39minternal_values()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_group_2(df):\n",
    "  groups = []\n",
    "  group_ids = df['GroupId'].unique().tolist()\n",
    "    \n",
    "  for group_id in group_ids:\n",
    "    \n",
    "    group_df = df[df['GroupId'] == group_id]\n",
    "    \n",
    "    at_least_one_missing_destination_planet = group_df['Destination'].isna().any()\n",
    "    only_one_distinct_destination_planet = group_df['Destination'].dropna().nunique() == 1    \n",
    "    only_one_distinct_home_planet = group_df['HomePlanet'].dropna().nunique() == 1\n",
    "    only_one_distinct_last_name = group_df['LastName'].dropna().nunique() == 1\n",
    "    \n",
    "    if (\n",
    "        at_least_one_missing_destination_planet and\n",
    "        only_one_distinct_home_planet and\n",
    "        only_one_distinct_destination_planet and\n",
    "        only_one_distinct_last_name\n",
    "    ):\n",
    "        groups.append(group_df)\n",
    "    \n",
    "  return groups\n",
    "\n",
    "print('number of remaining samples where destination is missing:',df['Destination'].isna().sum())\n",
    "\n",
    "groups = get_group_2(df)\n",
    "print(f'number of groups found: {len(groups)}')\n",
    "\n",
    "while groups:\n",
    "    group_df = groups.pop()\n",
    "    destination_planets = group_df['Destination'].dropna().unique().tolist()\n",
    "    if len(destination_planets) != 1:\n",
    "        raise ValueError(len(destination_planets))\n",
    "    df.loc[group_df['Destination'].isna().index, 'Destination'] = destination_planets[0]\n",
    "    \n",
    "print('number of remaining samples where destination is missing after imputation:',df['Destination'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute missing VIPS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['HomePlanet'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(df[df['HomePlanet'] == 'Earth']['VIP'].value_counts())\n",
    "display(df[df['HomePlanet'] == 'Mars']['VIP'].value_counts())\n",
    "display(df[df['HomePlanet'] == 'Europa']['VIP'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "- We can see nobody on earth is a VIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[(df['HomePlanet'] == 'Earth') & (df['VIP'].isna()), 'VIP'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "To determine which categorical features to impute next, lets attempt to rank our categorical features importance with respect to the target using regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(df.info())\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your data is in a DataFrame called 'df'\n",
    "categorical_features = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Side']  # Replace with your categorical feature names\n",
    "continuous_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'GroupId', 'Num', 'PersonId']\n",
    "target_feature = 'Transported'  # Replace with the name of your target variable\n",
    "\n",
    "temp_df = df[categorical_features + continuous_features + [target_feature]]\n",
    "\n",
    "temp_df = pd.get_dummies(temp_df, columns=categorical_features)\n",
    "\n",
    "X = temp_df.drop(columns=[target_feature])\n",
    "y = temp_df[target_feature]\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X, y)\n",
    "\n",
    "importance_df = pd.DataFrame({'Feature': rf_classifier.feature_names_in_, 'Importance': rf_classifier.feature_importances_})\n",
    "\n",
    "# Sort the features by importance in descending order\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance (Random Forest) for Transported')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- Continuous features have the largest impact on the target.\n",
    "- Unexpectedly, 'Num', and 'GroupId' have a large impact on the target.\n",
    "- The categorical feature that has the largest impact on the target is CryoSleep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cryosleep Analysis\n",
    "- Let's try to glean information about cryosleep for imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "did_cryo = df[df['CryoSleep'] == True]\n",
    "no_cryo = df[df['CryoSleep'] == False]\n",
    "\n",
    "spending_money_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "display(\"DID CRYO\")\n",
    "display(did_cryo.head())\n",
    "display(did_cryo[spending_money_cols].describe())\n",
    "display('-----------')\n",
    "display('NO CRYO')\n",
    "display(no_cryo.head())\n",
    "display(no_cryo[spending_money_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- We can see that people who do cryo never spend any money. This can help us impute missing cryo data and missing money data. \n",
    "- We can see that people who did CRYO and were VIP were all from Europa Home planet. We can quickly impute this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation: CryoSleep and Shopping Data\n",
    "- We discussed earlier that shopping data is a good proxy for determining if someone did cryosleep or not. Lets begin by imputing shopping data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Imputation:***\n",
    "- If someone did cryosleep, we know they didn't spend any money. So, for any sample where shopping data is missing, if the person did cryosleep, we will impute a 0.0.\n",
    "- If cryosleep data is missing, and we see the person spent money, we know they didn't do cryosleep. That is, if a person spent money and cryosleep is missing, we will fill a False.df[(df['CryoSleep'] == True) & (df[spending_money_cols].sum(axis=1))]spending_money_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spending_money_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['CryoSleep'] == True) & (df[spending_money_cols].sum(axis=1) > 0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask = df[(df['CryoSleep'] == True) & (df[spending_money_cols].isna().any(axis=1))].index\n",
    "df.loc[mask, spending_money_cols] = 0.0\n",
    "\n",
    "mask = df[(df['CryoSleep'].isna()) & (df[spending_money_cols].sum(axis=1) > 0.0)].index\n",
    "df.loc[mask, 'CryoSleep'] = False\n",
    "\n",
    "df[(df['CryoSleep'] == True) & (df[spending_money_cols].sum(axis=1) > 0.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering: TotalSpent and SpentMoney column\n",
    "- Spending money appears to be a good proxy for certain information. We can create a simple continuous feature that simply tells how much money, if any, was spent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['TotalSpent'] = df[spending_money_cols].sum(axis=1)\n",
    "df.loc[df[spending_money_cols].sum(axis=1) > 0, 'SpentMoney'] = True\n",
    "df.loc[(df[spending_money_cols].notna().all(axis=1)) & (df[spending_money_cols].sum(axis=1) == 0), 'SpentMoney'] = False\n",
    "df[df[spending_money_cols].isna().any(axis=1) & (df['TotalSpent'] == 0)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Round II\n",
    "- We've imputed some data and engineered some new features. Lets check out the new feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming your data is in a DataFrame called 'df'\n",
    "categorical_features = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Side', 'SpentMoney']  # Replace with your categorical feature names\n",
    "continuous_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'GroupId', 'Num', 'PersonId', 'TotalSpent']\n",
    "target_feature = 'Transported'  # Replace with the name of your target variable\n",
    "\n",
    "temp_df = df[categorical_features + continuous_features + [target_feature]]\n",
    "\n",
    "temp_df = pd.get_dummies(temp_df, columns=categorical_features)\n",
    "\n",
    "X = temp_df.drop(columns=[target_feature])\n",
    "y = temp_df[target_feature]\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=1337)\n",
    "rf_classifier.fit(X, y)\n",
    "\n",
    "importance_df = pd.DataFrame({'Feature': rf_classifier.feature_names_in_, 'Importance': rf_classifier.feature_importances_})\n",
    "\n",
    "# Sort the features by importance in descending order\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance (Random Forest) for Transported')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Missing features\")\n",
    "display(df.isna().sum())\n",
    "print(len(df[categorical_features + continuous_features + [target_feature]].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Training Block\n",
    "categorical_features = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck' , 'Side', 'SpentMoney']\n",
    "continuous_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck' , 'GroupId', 'PersonId', 'Num', 'TotalSpent']\n",
    "target = 'Transported'\n",
    "\n",
    "df_train = df[categorical_features + continuous_features + [target]].dropna()\n",
    "df_train = pd.get_dummies(df_train, columns=['HomePlanet', 'Destination'])\n",
    "df_train = pd.get_dummies(df_train, columns=['Deck', 'Side'], drop_first=True)\n",
    "X = df_train.drop(columns=['Transported'])\n",
    "y = df_train[['Transported']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X[continuous_features] = scaler.fit_transform(X[continuous_features])\n",
    "\n",
    "display(X.head())\n",
    "display(y.head())\n",
    "                   \n",
    "gb_param_grid = {\n",
    "}\n",
    "\n",
    "random_forest_classifier_grid =  {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 5, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "gridcv = GridSearchCV(RandomForestClassifier(random_state=42), param_grid=random_forest_classifier_grid, n_jobs=14, scoring='accuracy', cv=5, refit=True)\n",
    "grid_search = gridcv.fit(X,y.values.ravel())\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters: \", best_params)\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best score: \", best_score)\n",
    "best_estimator = grid_search.best_estimator_\n",
    "print(\"Best estimator: \", best_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3324462483.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[35], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(len(df.dropna())\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "print(len(df.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CryoSleep', 'VIP', 'Age', 'RoomService', 'FoodCourt', 'ShoppingMall',\n",
      "       'Spa', 'VRDeck', 'GroupId', 'PersonId', 'Num', 'Transported',\n",
      "       'HomePlanet_Earth', 'HomePlanet_Europa', 'HomePlanet_Mars',\n",
      "       'Destination_55 Cancri e', 'Destination_PSO J318.5-22',\n",
      "       'Destination_TRAPPIST-1e', 'Deck_B', 'Deck_C', 'Deck_D', 'Deck_E',\n",
      "       'Deck_F', 'Deck_G', 'Deck_T', 'Side_S'],\n",
      "      dtype='object')\n",
      "           Age  RoomService  FoodCourt  ...   GroupId  PersonId       Num\n",
      "0     0.695304    -0.347478  -0.286582  ... -1.742774 -0.491244 -1.165884\n",
      "1    -0.337303    -0.179177  -0.281206  ... -1.742400 -0.491244 -1.165884\n",
      "2     2.003272    -0.281084   1.849635  ... -1.742026 -0.491244 -1.165884\n",
      "3     0.282261    -0.347478   0.479851  ... -1.742026  0.458401 -1.165884\n",
      "4    -0.888027     0.120369  -0.244766  ... -1.741652 -0.491244 -1.163935\n",
      "...        ...          ...        ...  ...       ...       ...       ...\n",
      "8688  0.832985    -0.347478   3.786925  ...  1.727247 -0.491244 -0.974835\n",
      "8689 -0.750346    -0.347478  -0.286582  ...  1.727995 -0.491244  1.756386\n",
      "8690 -0.199622    -0.347478  -0.286582  ...  1.728369 -0.491244  1.758335\n",
      "8691  0.213421    -0.347478   0.340065  ...  1.728743 -0.491244  0.019400\n",
      "8692  1.039506    -0.152928   2.513917  ...  1.728743  0.458401  0.019400\n",
      "\n",
      "[6795 rows x 9 columns]\n",
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n",
      "Best parameters:  {'learning_rate': 0.01, 'loss': 'exponential', 'max_depth': 3, 'n_estimators': 200}\n",
      "Best score:  0.7798381162619574\n",
      "Best estimator:  GradientBoostingClassifier(learning_rate=0.01, loss='exponential',\n",
      "                           n_estimators=200, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "!python3 ./main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Unscaled:\n",
    "# .779838116"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
